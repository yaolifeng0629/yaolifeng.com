**让长文本处理又快又好的新方法：NSA 原⽣稀疏注意⼒机制**

最近，DeepSeek 在优化大语言模型的长文本处理能力上取得了新突破。

传统模型在处理一篇小说或整个代码库时，常因计算量过大而卡顿。

DeepSeek 提出的**NSA（原⽣稀疏注意⼒机制）**，像给模型装上了“智能滤网”，既能抓住关键信息，又大幅降低了计算负担，速度直接提升 11.6 倍。

### 一、问题：长文本处理为何如此吃力？

传统注意力机制（Full Attention）要求模型在处理每个词时，都要和之前所有词做关联计算。

想象一下，如果一篇文章有 6 万个词，模型需要做近 36 亿次计算！这种“全员参与”的模式虽然全面，但效率极低。尤其在实际应用中，解码一段长文本可能要花 70%的时间在注意力计算上（图 1 右）。

![图 1](https://qncdn.mopic.mozigu.net/work/143/25/7e83d3fa0a6d425d/image.png)
▲ 图 1 | 左：NSA 在各项任务中表现不输全注意力模型；右：处理 6.4 万长度文本时，NSA 解码速度提升 11.6 倍。

### 二、解决思路：像人类阅读一样“抓重点”

人类阅读长文时会自然跳过无关段落，只关注关键部分。

NSA 模仿这一机制，设计了**三层注意力筛网**（图 2）：

1. **压缩层**：把每 32 个词压缩成一个“段落梗概”
2. **精选层**：动态筛选 64 个最重要的词块
3. **滑动窗**：始终关注最近的 512 个词，防止漏掉局部信息

![图 2](https://qncdn.mopic.mozigu.net/work/143/25/7e83d3fa0a6d425d/image-1.png)
▲ 图 2 |三种注意力模式分工合作，绿色区域代表实际计算部分

这种设计让计算量骤减——原本需要处理 6 万个词，现在只需关注约 5 千个关键点，同时通过硬件级优化（如连续内存读取、Tensor Core 加速），让理论提速真正落地。

### 三、实战表现：又快又聪明的“双料冠军”

DeepSeek 在 270 亿参数模型上做了全面测试：

-   **常规任务**：在数学推理（GSM8K）、代码生成（HumanEval）等 9 项测试中，NSA7 项领先（表 1）
    ![表 1](https://qncdn.mopic.mozigu.net/work/143/25/7e83d3fa0a6d425d/image-3.png)
    ▲ 表 1 | 全注意⼒基线与 NSA 的预训练性能⽐较
-   **长文本检索**：在 6.4 万字的“大海捞针”测试中，NSA 实现了 100%准确率（图 5）
    ![图 5](https://qncdn.mopic.mozigu.net/work/143/25/7e83d3fa0a6d425d/image-2.png)
    ▲ 图 5 | 64k 上下⽂⻓度的上下⽂位置上的⼤海捞针检索准确率
-   **推理能力**：经过专项训练后，NSA 解决美国数学竞赛题的正确率比传统模型高出 60%（表 3）
    ![表 3](https://qncdn.mopic.mozigu.net/work/143/25/7e83d3fa0a6d425d/image-4.png)
    ▲ 表 3 | 监督微调后的基于 AIME 指令的评

更关键的是速度优势：

-   **训练提速**：处理 6.4 万长度文本时，前向计算提速 9 倍，反向传播提速 6 倍
-   **解码飞跃**：生成同样内容，内存读取量减少 90%，实际响应速度提升 11.6 倍

### 四、突破性创新：从“事后补救”到“原生设计”

现有方案多在模型训练完成后才启用稀疏计算，相当于给建好的房子拆墙开窗。

而 NSA 从一开始就让模型学习如何高效分配注意力：

-   **硬件对齐**：像拼乐高一样设计计算模块，完美匹配 GPU 的 Tensor Core 特性
-   **动态学习**：每个词块的筛选标准由模型自主调整，重要信息绝不漏网
-   **端到端训练**：支持从预训练到微调的全流程，避免后期“水土不服”

### 五、这技术能用来做什么？

试想这些场景：

-   程序员上传整个项目代码，AI 秒级理解架构并生成新功能
-   上传上百页的 PDF，AI 可以快速提取关键信息
-   游戏 NPC 记住玩家上千条对话历史，做出连贯反应

NSA 已在这些方向初步验证成功（表 2）。
![表 2](https://qncdn.mopic.mozigu.net/work/143/25/7e83d3fa0a6d425d/image-5.png)
▲ 表 2 | NSA 与 LongBench 上的基线之间的性能⽐较

未来，它可能成为处理长文本的“标配”技术，让大模型真正突破上下文长度的枷锁。
